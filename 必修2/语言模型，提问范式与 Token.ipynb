{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ece8e10-4f98-4710-abb3-6b98908e1bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 语言模型，提问范式与 Token\n",
    "# 在本章中，我们将和您分享大型语言模型（LLM）的工作原理、训练方式以及分词器（tokenizer）等细节对 LLM \n",
    "# 输出的影响。我们还将介绍 LLM 的提问范式（chat format），这是一种指定系统消息（system message）和\n",
    "# 用户消息（user message）的方式，让您了解如何利用这种能力。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd81608a-5191-431b-b1ef-ad4ce15f3ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install openai\n",
    "# !pip install langchain\n",
    "# !pip install --upgrade tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10ab68ba-255e-4abf-ae24-e7828a70fcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "def get_completion_gpt(prompt, model=\"gpt-4o-mini\", temperature=0): \n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}] # 消息队列\n",
    "    client = OpenAI()\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature, # 值越低则输出文本随机性越低\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bca9c5f-e2e8-4c6c-bb88-df3d674e11ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 尝试向模型提问并得到结果\n",
    "# LLM 可以通过使用监督学习来构建，通过不断预测下一个词来学习。 并且，给定一个大的训练集，有数百亿甚至更多的词，你可\n",
    "# 以创建一个大规模的训练集，你可以从一 句话或一段文本的一部分开始，反复要求语言模型学习预测下一个词是什么\n",
    "# LLM 主要分为两种类型：基础语言模型（Base LLM）和越来越受欢迎的指令微调语言模型（Instruction Tuned LLM）。基础\n",
    "# 语言模型通过反复预测下一个词来训练，因此如果我们给它一个 Prompt，比如“从前有一只独角兽”，它可能通过逐词预测来完成一\n",
    "# 个关于独角兽在魔法森林中与其他独角兽朋友们生活的故事。\n",
    "# 然而，这种方法的缺点是，如果您给它一个 Prompt，比如“中国的首都是哪里？”，很可能它数据中有一段互联网上关于中国的测验问\n",
    "# 题列表。这时，它可能会用“中国最大的城市是什么？中国的人口是多少？”等等来回答这个问题。但实际上，您只是想知道中国的首都是\n",
    "# 什么，而不是列举所有这些问题。然而，指令微调语言模型会尝试遵循 Prompt，并给出“中国的首都是北京”的回答。\n",
    "# 那么，如何将基础语言模型转变为指令微调语言模型呢？这就是训练一个指令微调语言模型（例如ChatGPT）的过程。首先，您需要在大量数\n",
    "# 据上训练基础语言模型，因此需要数千亿个单词，甚至更多。这个过程在大型超级计算系统上可能需要数月时间。训练完基础语言模型后，您会\n",
    "# 通过在一小部分示例上进行进一步的训练，使模型的输出符合输入的指令。例如，您可以请承包商帮助您编写许多指令示例，并对这些指令的正\n",
    "# 确回答进行训练。这样就创建了一个用于微调的训练集，让模型学会在遵循指令的情况下预测下一个词是什么。\n",
    "# 之后，为了提高语言模型输出的质量，常见的方法是让人类对许多不同输出进行评级，例如是否有用、是否真实、是否无害等。然后，您可以进\n",
    "# 一步调整语言模型，增加生成高评级输出的概率。这通常使用强化学习中的人类反馈（RLHF）技术来实现。相较于训练基础语言模型可能需要\n",
    "# 数月的时间，从基础语言模型到指令微调语言模型的转变过程可能只需要数天时间，使用较小规模的数据集和计算资源。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cbcbba-07d3-45fb-aa43-cdd61e8a8df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = get_completion_gpt(\"What is the capital of China?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32419ab3-2305-4f15-9416-6d972fb71171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中国的首都是北京。\n"
     ]
    }
   ],
   "source": [
    "response = get_completion_gpt(\"中国的首都是哪里？\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cf46cd-16dc-4ffc-aca7-d5a038af9b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokens\n",
    "# 到目前为止对 LLM 的描述中，我们将其描述为一次预测一个单词，但实际上还有一个更重要的技术细节。即 LLM \n",
    "# 实际上并不是重复预测下一个单词，而是重复预测下一个 token 。当 LLM 接收到输入时，它将将其转换为一系\n",
    "# 列 token，其中每个 token 都代表常见的字符序列。例如，对于 \"Learning new things is fun!\" 这句话\n",
    "# ，每个单词都被转换为一个 token ，而对于较少使用的单词，如 \"Prompting as powerful developer tool\"，\n",
    "# 单词 \"prompting\" 会被拆分为三个 token，即\"prom\"、\"pt\"和\"ing\"。(子词单元)\n",
    "# 当您要求 ChatGPT 颠倒 \"lollipop\" 的字母时，由于分词器（tokenizer） 将 \"lollipop\" 分解为三个 \n",
    "# token，即 \"l\"、\"oll\"、\"ipop\"，因此 ChatGPT 难以正确输出字母的顺序。您可以通过在字母之间添加连字符或\n",
    "# 空格的方式，使分词器将每个字母分解为单独的 token，从而帮助 ChatGPT 更好地认识单词中的每个字母并正确输出它们。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1f9e127-eb85-4d80-bb9c-f05aec5e8fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reversing the letters in \"lollipop\" gives you \"popillol.\"\n"
     ]
    }
   ],
   "source": [
    "# 为了更好展示效果，这里就没有翻译成中文的 Prompt\n",
    "# 注意这里的字母翻转出现了错误，吴恩达老师正是通过这个例子来解释 token 的计算方式\n",
    "# GPT-4 在处理这个任务时已经改进了错误。吴恩达的例子反映了早期版本模型可能存在的局限性，而 GPT-4 \n",
    "# 在 token 切分和任务理解上已经显著优化。所以这里反转正确\n",
    "response = get_completion_gpt(\"Take the letters in lollipop \\\n",
    "and reverse them\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "748ce41f-15b1-48e8-a76a-7788f1e8f410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reversing the letters in \"l-o-l-l-i-p-o-p\" gives you \"p-o-p-i-l-l-o-l\".\n"
     ]
    }
   ],
   "source": [
    "response = get_completion_gpt(\"\"\"Take the letters in l-o-l-l-i-p-o-p and reverse them\"\"\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e01fc8-e098-4e1e-9126-c8f086539ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对于英文输入，一个 token 一般对应 4 个字符或者四分之三个单词；对于中文输入，一个 token 一般对应一个或半个词。\n",
    "# 不同模型有不同的 token 限制，需要注意的是，这里的 token 限制是输入的 Prompt 和输出的 completion 的 token \n",
    "# 数之和，因此输入的 Prompt 越长，能输出的 completion 的上限就越低。\n",
    "# ChatGPT3.5-turbo 的 token 上限是 4096。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782b756a-507b-4285-9d38-da3bdd09835a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function 辅助函数 (提问范式)\n",
    "# 下面是课程中用到的辅助函数。 下图是 OpenAI 提供的一种提问范式，接下来吴恩达老师就是在演示如何利用这种范式进行更好的提问"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c8a1a92-435e-4017-b005-99c1a8d7aac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System 信息用于指定模型的规则，例如设定、回答准则等，而 assistant 信息就是让模型完成的具体指令\n",
    "# 系统信息相当于给模型设定个框框,因为模型训练时应该是基于大量数据,这时设置到一定范围\n",
    "# 而助理信息是模型基于多轮对话的上下文生成的回复\n",
    "client = OpenAI()\n",
    "def get_completion_from_messages(messages, \n",
    "                                 model=\"gpt-4o-mini\", \n",
    "                                 temperature=0.5, \n",
    "                                 max_tokens=500):\n",
    "    '''\n",
    "    封装一个支持更多参数的自定义访问 gpt-4o-mini 的函数\n",
    "    参数: \n",
    "    messages: 这是一个消息列表，每个消息都是一个字典，包含 role(角色）和 content(内容)。角色可以是'system'、'user' 或 'assistant’，内容是角色的消息。\n",
    "    model: 调用的模型，默认为 gpt-4o-mini(ChatGPT)，有内测资格的用户可以选择 gpt-4\n",
    "    temperature: 这决定模型输出的随机程度，默认为0.5，增加温度会使输出更随机。\n",
    "    max_tokens: 这决定模型输出的最大的 token 数。\n",
    "    '''\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature, # 这决定模型输出的随机程度\n",
    "        max_tokens=max_tokens, # 这决定模型输出的最大的 token 数\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f341a0-4959-47ea-acc7-74ea7965cc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages =  [  \n",
    "{'role':'system', \n",
    " 'content':\"\"\"You are an assistant who\\\n",
    " responds in the style of Dr Seuss.\"\"\"},    \n",
    "{'role':'user', \n",
    " 'content':\"\"\"write me a very short poem\\\n",
    " about a happy carrot\"\"\"},  \n",
    "] \n",
    "response = get_completion_from_messages(messages, temperature=1)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ae35ca4-259b-44b3-9714-2e9101bf457c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在蓝色的海洋中，游着一条小鲸，  \n",
      "她快乐地嬉戏，像风中的纸鸥翎。  \n",
      "波浪轻轻拍打，水花四处飞溅，  \n",
      "小鲸鱼唱歌，声声如晨钟响叮当。\n",
      "\n",
      "“来吧，朋友们，跟我一起舞动！  \n",
      "在阳光的照耀下，让心情永远熏染！  \n",
      "跳跃的海豚，欢快而灵动，  \n",
      "我们在海底，编织梦想的画卷！”\n",
      "\n",
      "她游过珊瑚，嬉戏在泡沫中，  \n",
      "螃蟹们欢呼，海星们来捧场。  \n",
      "小鲸鱼的笑声在海里荡漾，  \n",
      "快乐的旋律，永不消亡！\n"
     ]
    }
   ],
   "source": [
    "messages =  [  \n",
    "{'role':'system', \n",
    " 'content':'你是一个助理，并以 Seuss 苏斯博士的风格作出回答。'},    \n",
    "{'role':'user', \n",
    " 'content':'就快乐的小鲸鱼为主题给我写一首短诗'},  \n",
    "] \n",
    "response = get_completion_from_messages(messages, temperature=1)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7c3e72-7f3b-413c-a2d1-1f9a0b48df84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# length\n",
    "messages =  [  \n",
    "{'role':'system',\n",
    " 'content':'All your responses must be \\\n",
    "one sentence long.'},    \n",
    "{'role':'user',\n",
    " 'content':'write me a story about a happy carrot'},  \n",
    "] \n",
    "response = get_completion_from_messages(messages, temperature =1)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5c8df62-ad07-4f9e-a1cf-4f9e3740b174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "从前有一只快乐的小鲸鱼，它每天在大海里唱歌、跳舞，与朋友们一起探险，享受海洋的美丽，直到有一天它遇到了一个迷路的小海龟，决定帮助它找到回家的路。\n"
     ]
    }
   ],
   "source": [
    "# 长度控制\n",
    "messages =  [  \n",
    "{'role':'system',\n",
    " 'content':'你的所有答复只能是一句话'},    \n",
    "{'role':'user',\n",
    " 'content':'写一个关于快乐的小鲸鱼的故事'},  \n",
    "] \n",
    "response = get_completion_from_messages(messages, temperature =1)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d67cf5e-5356-415d-ba35-170fff4166cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined\n",
    "messages =  [  \n",
    "{'role':'system',\n",
    " 'content':\"\"\"You are an assistant who \\\n",
    "responds in the style of Dr Seuss. \\\n",
    "All your responses must be one sentence long.\"\"\"},    \n",
    "{'role':'user',\n",
    " 'content':\"\"\"write me a story about a happy carrot\"\"\"},\n",
    "] \n",
    "response = get_completion_from_messages(messages, \n",
    "                                        temperature =1)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "864c0ce5-3f53-486d-8b2a-c4f91a7240c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在波光粼粼的蓝色海洋里，有一只快乐的小鲸鱼，唱着欢快的歌，和朋友们一起旋转、跳跃，享受着无忧无虑的海洋乐趣！\n"
     ]
    }
   ],
   "source": [
    "# 以上结合\n",
    "messages =  [  \n",
    "{'role':'system',\n",
    " 'content':'你是一个助理， 并以 Seuss 苏斯博士的风格作出回答，只回答一句话'},    \n",
    "{'role':'user',\n",
    " 'content':'写一个关于快乐的小鲸鱼的故事'},\n",
    "] \n",
    "response = get_completion_from_messages(messages, temperature =1)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ac2c02-bba0-4483-93ee-81a46975c89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "664429d7-b43d-4204-b57b-eacf1ecc95a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion_and_token_count(messages, \n",
    "                                   model=\"gpt-4o-mini\", \n",
    "                                   temperature=0, \n",
    "                                   max_tokens=500):\n",
    "    \"\"\"\n",
    "    使用 OpenAI 的 gpt-4o-mini 模型生成聊天回复，并返回生成的回复内容以及使用的 token 数量。\n",
    "    参数:\n",
    "    messages: 聊天消息列表。\n",
    "    model: 使用的模型名称。默认为\"gpt-4o-mini\"。\n",
    "    temperature: 控制生成回复的随机性。值越大，生成的回复越随机。默认为 0。\n",
    "    max_tokens: 生成回复的最大 token 数量。默认为 500。\n",
    "    返回:\n",
    "    content: 生成的回复内容。\n",
    "    token_dict: 包含'prompt_tokens'、'completion_tokens'和'total_tokens'的字典，分别表示提示的 \n",
    "    token 数量、生成的回复的 token 数量和总的 token 数量。\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature, \n",
    "        max_tokens=max_tokens,\n",
    "    )\n",
    "    content = response.choices[0].message.content\n",
    "    token_dict = {\n",
    "    'prompt_tokens':response.usage.prompt_tokens,\n",
    "    'completion_tokens':response.usage.completion_tokens,\n",
    "    'total_tokens':response.usage.total_tokens\n",
    "    }\n",
    "    return content, token_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e98199f1-7ffb-4afd-a20c-8ff251040c43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'在波光粼粼的蓝色海洋里，有一只快乐的小鲸鱼，唱着欢快的歌，和朋友们一起旋转、跳跃，享受着无忧无虑的海洋乐趣！'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2050dd1b-4a39-4d5f-9b59-f4e7f4fbfa23",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "{'role':'system', \n",
    " 'content':\"\"\"You are an assistant who responds\\\n",
    " in the style of Dr Seuss.\"\"\"},    \n",
    "{'role':'user',\n",
    " 'content':\"\"\"write me a very short poem \\ \n",
    " about a happy carrot\"\"\"},  \n",
    "] \n",
    "response, token_dict = get_completion_and_token_count(messages,temperature=0.7)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e61a5749-2cb6-435f-b3de-10b6463254a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt_tokens': 46, 'completion_tokens': 167, 'total_tokens': 213}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e822b47a-6407-4162-a1de-89d1bd5d688a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在蓝色的海洋，波浪轻轻摇，  \n",
      "有一只小鲸鱼，快乐又骄傲。  \n",
      "它在水中跳跃，像星星在舞，  \n",
      "喷出水花，闪烁着光芒如珠。\n",
      "\n",
      "“嗨，朋友们！”它欢快地叫，  \n",
      "“来和我一起，畅游这海潮！  \n",
      "我们在珊瑚间，嬉戏又玩耍，  \n",
      "在阳光下游弋，快乐无比呀！”\n",
      "\n",
      "小鲸鱼的笑声，传遍每个角落，  \n",
      "在海洋的怀抱，幸福永不落。  \n",
      "无论风浪如何，它总是微笑，  \n",
      "因为在这大海，它是最快乐的宝！\n"
     ]
    }
   ],
   "source": [
    "messages =  [  \n",
    "{'role':'system', \n",
    " 'content':'你是一个助理， 并以 Seuss 苏斯博士的风格作出回答。'},    \n",
    "{'role':'user', \n",
    " 'content':'就快乐的小鲸鱼为主题给我写一首短诗'},  \n",
    "] \n",
    "response, token_dict = get_completion_and_token_count(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8063edbb-04c8-4aaa-9adf-fc29efd86049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt_tokens': 46, 'completion_tokens': 167, 'total_tokens': 213}\n"
     ]
    }
   ],
   "source": [
    "print(token_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5561c47-6a34-4d3e-a74f-95eb0f1eedb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在蓝色的海洋里，快乐的小鲸鱼，  \n",
      "跳跃翻滚，欢声笑语，真是太美妙！  \n",
      "它唱着歌，水花四溅，  \n",
      "每个波浪都在为它欢呼，真是无比快乐的时光！\n"
     ]
    }
   ],
   "source": [
    "messages =  [  \n",
    "{'role':'system', \n",
    " 'content':'你是一个助理， 并以 Seuss 苏斯博士的风格作出回答。只回答一句话。'},    \n",
    "{'role':'user', \n",
    " 'content':'就快乐的小鲸鱼为主题给我写一首短诗'},  \n",
    "] \n",
    "response, token_dict = get_completion_and_token_count(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6994c1bb-5d03-4eb2-a91b-829bbd54bfb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt_tokens': 51, 'completion_tokens': 64, 'total_tokens': 115}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d89b02d-2ffa-4b4f-b8a8-0c05bc944505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最后，我们认为 Prompt 对 AI 应用开发的革命性影响仍未得到充分重视。在传统的监督机器学习工作流中，如果想要构建一\n",
    "# 个可以将餐厅评论分类为正面或负面的分类器，首先需要获取一大批带有标签的数据，可能需要几百个，这个过程可能需要几周，\n",
    "# 甚至一个月的时间。接着，您需要在这些数据上训练一个模型，找到一个合适的开源模型，并进行模型的调整和评估，这个阶段\n",
    "# 可能需要几天、几周，甚至几个月的时间。最后，您可能需要使用云服务来部署模型，将模型上传到云端，并让它运行起来，才能\n",
    "# 最终调用您的模型。整个过程通常需要一个团队数月时间才能完成。\n",
    "# 相比之下，使用基于 Prompt 的机器学习方法，当您有一个文本应用时，只需提供一个简单的 Prompt 就可以了。这个过程可能\n",
    "# 只需要几分钟，如果需要多次迭代来得到有效的 Prompt 的话，最多几个小时即可完成。在几天内（尽管实际情况通常是几个小时）\n",
    "# ，您就可以通过 API 调用来运行模型，并开始使用。一旦您达到了这个步骤，只需几分钟或几个小时，就可以开始调用模型进行推\n",
    "# 理。因此，以前可能需要花费六个月甚至一年时间才能构建的应用，现在只需要几分钟或几个小时，最多是几天的时间，就可以使用 \n",
    "# Prompt 构建起来。这种方法正在极大地改变 AI 应用的快速构建方式。\n",
    "# 需要注意的是，这种方法适用于许多非结构化数据应用，特别是文本应用，以及越来越多的视觉应用，尽管目前的视觉技术仍在发展中。\n",
    "# 但它并不适用于结构化数据应用，也就是那些处理 Excel 电子表格中大量数值的机器学习应用。然而，对于适用于这种方法的应用，\n",
    "# AI 组件可以被快速构建，并且正在改变整个系统的构建工作流。构建整个系统可能仍然需要几天、几周或更长时间，但至少这部分可以\n",
    "# 更快地完成。\n",
    "# 下一个章中，我们将展示如何利用这些组件来评估客户服务助手的输入。 这将是本课程中构建在线零售商客户服务助手的更完整示例的一部分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02b42f5-044e-408c-8ffd-4e73b9bacac5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
